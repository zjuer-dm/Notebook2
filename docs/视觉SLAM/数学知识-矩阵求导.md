# 矩阵求导
## 简介与核心思想

矩阵求导（Matrix Calculus）是多元微积分在矩阵和向量运算中的一种表达方式，它将复杂的多元函数求导过程简洁地表示为矩阵和向量的操作。这在机器学习、深度学习、优化理论等领域至关重要，尤其是在处理梯度下降、损失函数优化、反向传播等问题时。

### 1. 布局约定 (Layout Convention)

矩阵求导有两种布局约定：**分子布局 (Numerator Layout)** 和 **分母布局 (Denominator Layout)**。两者在结果上互为转置。在机器学习领域，**分母布局更为常见**，因为它与梯度下降等算法的结合更自然。

**本指南将统一采用分母布局 (Denominator Layout)。**

分母布局的核心思想是：导数的形状由求导变量（分母）决定。
- 如果对一个列向量 $x$ 求导，结果的形状将与 $x$ 的转置（行向量）相关，或者说结果的每一行是对 $x^T$ 的一个元素求导。
- **关键示例**: 设 $y = Ax$，其中 $A$ 是 $m \times n$ 矩阵, $x$ 是 $n \times 1$ 列向量, $y$ 是 $m \times 1$ 列向量。
  - 在分母布局下，$\frac{\partial y}{\partial x}$ 的结果是一个 $n \times m$ 的矩阵，其形状由 $x^T$ (行向量) 和 $y$ (列向量) 决定，具体结果为 $A^T$。
  - $$\frac{\partial (Ax)}{\partial x} = A^T$$

---

## 2. 核心定义

#### a. 梯度 (Gradient): 标量对向量求导

函数 $f: \mathbb{R}^n \rightarrow \mathbb{R}$（输入为 $n$ 维向量，输出为标量），$f(x)$ 对 $x$ 的梯度是一个 **列向量**。

$$
\nabla_x f(x) = \frac{\partial f(x)}{\partial x} = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
$$

#### b. 雅可比矩阵 (Jacobian): 向量对向量求导

函数 $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$（输入为 $n$ 维向量，输出为 $m$ 维向量），$f(x)$ 对 $x$ 的雅可比矩阵是一个 $n \times m$ 的矩阵 (分母布局)。

$$
J = \frac{\partial f(x)}{\partial x} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_2}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_1} \\
\frac{\partial f_1}{\partial x_2} & \frac{\partial f_2}{\partial x_2} & \cdots & \frac{\partial f_m}{\partial x_2} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_1}{\partial x_n} & \frac{\partial f_2}{\partial x_n} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{bmatrix} = (\nabla_x f_1(x) \quad \nabla_x f_2(x) \quad \cdots \quad \nabla_x f_m(x))^T
$$

每一 **行** 是 $f$ 的所有输出分量对 **一个** 输入变量 $x_i$ 的偏导数。

#### c. 标量对矩阵求导

函数 $f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$（输入为 $m \times n$ 矩阵，输出为标量），$f(X)$ 对 $X$ 的导数是一个与 $X$ 形状相同的 $m \times n$ 矩阵。

$$
\frac{\partial f(X)}{\partial X} = \begin{bmatrix}
\frac{\partial f}{\partial X_{11}} & \frac{\partial f}{\partial X_{12}} & \cdots & \frac{\partial f}{\partial X_{1n}} \\
\frac{\partial f}{\partial X_{21}} & \frac{\partial f}{\partial X_{22}} & \cdots & \frac{\partial f}{\partial X_{2n}} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f}{\partial X_{m1}} & \frac{\partial f}{\partial X_{m2}} & \cdots & \frac{\partial f}{\partial X_{mn}}
\end{bmatrix}
$$

---

## 3. 常用求导法则 ("公式小抄")

以下公式均基于**分母布局**。变量含义：$x$ 是列向量, $A$ 是矩阵。

| 表达式 | 对 $x$ 的导数 | 注释 |
| :--- | :--- | :--- |
| $a^T x$ | $a$ | $a$ 是与 $x$ 维度相同的列向量 |
| $x^T a$ | $a$ | 同上 |
| $Ax$ | $A^T$ | 结果是雅可比矩阵 |
| $x^T A$ | $A$ | 结果是雅可比矩阵 |
| $x^T x$ | $2x$ | |
| $x^T A x$ | $(A + A^T)x$ | 如果 $A$ 是对称矩阵，则为 $2Ax$ |

| 表达式 | 对 $X$ 的导数 | 注释 |
| :--- | :--- | :--- |
| $\text{Tr}(AX)$ | $A^T$ | $\text{Tr}$ 表示迹 (Trace) |
| $\text{Tr}(XA)$ | $A^T$ | |
| $\text{Tr}(A^T X B)$ | $AB^T$ | |
| $\text{Tr}(X^T A X)$ | $A X + A^T X$ | |
| $\det(X)$ | $\det(X) (X^{-1})^T$ | $\det$ 表示行列式 (Determinant) |
| $\log(\det(X))$ | $(X^{-1})^T$ | |

### 3.1 迹技巧 (Trace Trick)

迹技巧是一个强大的工具，它利用迹的循环不变性 `Tr(ABC) = Tr(BCA) = Tr(CAB)` 来简化求导。

**核心思想**：将一个标量表达式（尤其是二次型）转化为迹的形式，然后利用迹的求导法则。
- 标量本身可以看作是 $1 \times 1$ 矩阵的迹，即 $z = \text{Tr}(z)$。
- 例如，在推导 $x^T A x$ 的导数时：
  1. $f(x) = x^T A x = \text{Tr}(x^T A x)$ (因为结果是标量)
  2. 利用循环不变性（将 $x$ 移动到前面）：$\text{Tr}(x^T A x) = \text{Tr}(A x x^T)$
  3. 使用迹的求导公式 $\frac{\partial \text{Tr}(AXB)}{\partial X} = A^T B^T$ 的变体来求解。

---

## 4. 链式法则 (Chain Rule)

链式法则是矩阵求导的灵魂，尤其是在反向传播中。

假设 $z = f(y)$ 且 $y = g(x)$。我们想求 $\frac{\partial z}{\partial x}$。

- **向量形式**:
  $$
  \frac{\partial z}{\partial x} = \frac{\partial y}{\partial x} \frac{\partial z}{\partial y}
  $$
  **维度分析 (分母布局)**:
  - $x \in \mathbb{R}^n$, $y \in \mathbb{R}^m$, $z \in \mathbb{R}^p$
  - $\frac{\partial z}{\partial x}$ 的维度是 $n \times p$
  - $\frac{\partial y}{\partial x}$ 的维度是 $n \times m$
  - $\frac{\partial z}{\partial y}$ 的维度是 $m \times p$
  - 维度匹配: $(n \times p) = (n \times m) \times (m \times p)$

### 4.1 应用：反向传播 (Backpropagation)

考虑一个简单的单层神经网络：$L = f(Wx)$，其中 $L$ 是标量损失，$W$ 是权重矩阵，$x$ 是输入向量。求损失对权重 $W$ 的导数 $\frac{\partial L}{\partial W}$。

1. **定义中间变量**: 设 $Z = Wx$。
2. **应用链式法则**:
   $$
   \frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial Z} \frac{\partial Z}{\partial W_{ij}} \quad (\text{逐元素形式})
   $$
3. **矩阵形式**:
   - 设 $\delta = \frac{\partial L}{\partial Z}$ (这是一个与 $Z$ 维度相同的向量)。
   - 我们需要计算 $\frac{\partial Z}{\partial W} = \frac{\partial (Wx)}{\partial W}$。
   - 最终可以推导出:
     $$
     \frac{\partial L}{\partial W} = \delta \cdot x^T
     $$
   这里的 $\delta$ 是从后一层网络传递过来的误差项，而 $x^T$ 是前一层的激活值（或输入）。这完美地体现了“误差项”与“前层激活”的乘积形式。

---

## 5. 二阶导数：海森矩阵 (Hessian Matrix)

海森矩阵是标量函数 $f: \mathbb{R}^n \rightarrow \mathbb{R}$ 的二阶导数，是一个 $n \times n$ 的对称矩阵。它是梯度的雅可比矩阵。

$$
H = \nabla_x^2 f(x) = \frac{\partial}{\partial x} \left( \frac{\partial f(x)}{\partial x} \right)^T = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

### 5.1 海森矩阵与凸性

海森矩阵在优化问题中用于判断函数的局部曲率和凸性。
- **正定 (Positive Definite)**: 如果在某点 $x_0$，$H(x_0)$ 是正定矩阵（所有特征值 $> 0$），则 $f(x)$ 在该点是一个局部极小值。
- **半正定 (Positive Semidefinite)**: 如果在整个定义域内 $H(x)$ 都是半正定矩阵（所有特征值 $\ge 0$），则 $f(x)$ 是一个**凸函数**。
- **负定/半负定**: 对应局部极大值和凹函数。

---

## 6. 推导示例 1：线性回归的正规方程 (Normal Equation)

**目标**: 找到参数 $\theta$ 最小化成本函数 $J(\theta) = \frac{1}{2m} \|X\theta - y\|^2$。为了简化，我们忽略常数 $\frac{1}{2m}$，最小化 $J(\theta) = \|X\theta - y\|^2$。

1. **将成本函数展开**:
   $$
   \begin{aligned}
   J(\theta) &= (X\theta - y)^T (X\theta - y) \\
   &= ( (X\theta)^T - y^T ) (X\theta - y) \\
   &= (\theta^T X^T - y^T) (X\theta - y) \\
   &= \theta^T X^T X \theta - \theta^T X^T y - y^T X \theta + y^T y
   \end{aligned}
   $$

2. **化简表达式**:
   - 注意 $\theta^T X^T y$ 是一个标量，所以它等于它的转置 $( \theta^T X^T y )^T = y^T (X^T)^T (\theta^T)^T = y^T X \theta$。
   - 因此，成本函数可以写为：
     $$
     J(\theta) = \theta^T (X^T X) \theta - 2y^T X \theta + y^T y
     $$

3. **对 $\theta$ 求梯度**:
   使用前面“公式小抄”中的法则：
   - $\frac{\partial (\theta^T A \theta)}{\partial \theta} = (A + A^T)\theta$。这里 $A = X^T X$，是**对称矩阵**，所以导数是 $2(X^T X)\theta$。
   - $\frac{\partial (b^T \theta)}{\partial \theta} = b$。这里 $b^T = -2y^T X$，所以 $b = -2(y^T X)^T = -2X^T y$。
   - $y^T y$ 是常数，导数为 0。
   
   将它们组合起来：
   $$
   \nabla_\theta J(\theta) = 2X^T X \theta - 2X^T y
   $$

4. **令梯度为零**:
   为了找到最小值，我们设置梯度为零向量：
   $$
   2X^T X \theta - 2X^T y = 0
   $$
   $$
   X^T X \theta = X^T y
   $$

5. **求解 $\theta$**:
   假设 $X^T X$ 是可逆的（这要求特征是线性无关的），我们两边都左乘 $(X^T X)^{-1}$：
   $$
   \theta = (X^T X)^{-1} X^T y
   $$
   这就是线性回归的正规方程解。这个推导过程完美地展示了矩阵求导在解决实际优化问题中的威力。

---

## 7. 进阶主题与特殊情况

#### a. Vec 算子与 Kronecker 积

在处理更复杂的矩阵求导时，`vec` 算子和 Kronecker 积是非常强大的工具。

- **Vec 算子 `vec(X)`**: 将一个 $m \times n$ 的矩阵 $X$ 按列堆栈成一个 $mn \times 1$ 的列向量。
  $$
  X = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \implies \text{vec}(X) = \begin{bmatrix} a \\ c \\ b \\ d \end{bmatrix}
  $$

- **Kronecker 积 `A ⊗ B`**: 这是一个将两个任意大小的矩阵相乘得到一个更大矩阵的运算。
  $$
  A \otimes B = \begin{bmatrix}
  A_{11}B & A_{12}B & \cdots \\
  A_{21}B & A_{22}B & \cdots \\
  \vdots & \vdots & \ddots
  \end{bmatrix}
  $$

这两个工具结合起来可以简化很多求导法则，例如：
- **重要恒等式**: $\text{vec}(AXB) = (B^T \otimes A) \text{vec}(X)$。
- **应用**: 这个恒等式可以将复杂的矩阵方程转换为简单的向量-矩阵方程，从而简化求导。例如，求 $Y = AXB$ 对 $X$ 的导数，可以先向量化 $dY = A(dX)B$，得到 $\text{vec}(dY) = (B^T \otimes A) \text{vec}(dX)$，由此可知导数（雅可比矩阵）为 $B^T \otimes A$。

#### b. 对称矩阵求导

当求导变量 $X$ 是一个对称矩阵时，情况会变得特殊，因为 $X$ 的元素不再是完全独立的（$X_{ij} = X_{ji}$）。

- **一般法则**: 如果 $f(X)$ 是一个标量函数，并且 $X$ 是对称的，那么其导数公式为：
  $$
  \frac{\partial f(X)}{\partial X} = \left[ \frac{\partial f(X)}{\partial X} \right]_{\text{非对称}} + \left[ \frac{\partial f(X)}{\partial X} \right]_{\text{非对称}}^T - \text{diag}\left( \left[ \frac{\partial f(X)}{\partial X} \right]_{\text{非对称}} \right)
  $$
  其中 `[ ]_非对称` 表示先假设 $X$ 不是对称矩阵进行求导，`diag(M)` 表示取矩阵 `M` 的对角线元素构成一个对角矩阵。

- **示例**: 求 $\log(\det(X))$ 对对称矩阵 $X$ 的导数。
  1. **非对称情况**: 我们已经知道导数是 $(X^{-1})^T$。
  2. **应用对称法则**:
     - $(X^{-1})^T + ((X^{-1})^T)^T - \text{diag}((X^{-1})^T)$
     - $= X^{-1} + X^{-1} - \text{diag}(X^{-1})$ (因为 X 对称，所以 X⁻¹ 也对称)
     - $= 2X^{-1} - \text{diag}(X^{-1})$

#### c. 复数矩阵求导 (Wirtinger Calculus)

在信号处理和某些机器学习领域，会遇到对复数变量求导的问题。由于复数函数通常不是解析的（不满足柯西-黎曼条件），传统的导数定义不适用。Wirtinger Calculus 通过将函数 $f(z)$ 视为 $f(z, z^*)$ 的二元函数（其中 $z^*$ 是 $z$ 的共轭）来解决这个问题。

- **核心思想**: 分别对 $z$ 和 $z^*$ 求偏导，即使它们并非独立。
  - $\frac{\partial f}{\partial z} = \frac{1}{2} \left( \frac{\partial f}{\partial x} - i \frac{\partial f}{\partial y} \right)$
  - $\frac{\partial f}{\partial z^*} = \frac{1}{2} \left( \frac{\partial f}{\partial x} + i \frac{\partial f}{\partial y} \right)$
- 在机器学习优化中，我们通常只关心梯度 $\nabla_z f = 2 \frac{\partial f}{\partial z^*}$。

---

## 8. 推导示例 2：Softmax 回归的梯度

**目标**: 对多分类问题，我们使用 Softmax 函数计算概率，并用交叉熵作为损失函数。求损失函数对模型参数 $W$ 的梯度。

- **模型**:
  - 输入 $x_i$ (列向量, $D \times 1$)
  - 线性得分: $z_i = Wx_i$ ($W$ 是 $C \times D$ 矩阵, $C$ 是类别数)
  - Softmax 概率: $p_i = \text{softmax}(z_i)$，其第 $j$ 个元素为 $p_{ij} = \frac{e^{z_{ij}}}{\sum_{k=1}^C e^{z_{ik}}}$
  - 损失函数 (交叉熵): $L_i = - \sum_{j=1}^C y_{ij} \log(p_{ij}) = - y_i^T \log(p_i)$ ($y_i$ 是 one-hot 编码的真实标签向量)

**推导步骤**:
1.  **对 $z_i$ 求梯度**: 我们需要计算 $\frac{\partial L_i}{\partial z_i}$。这是一个向量对向量的求导，最终结果是一个列向量。
    - 使用链式法则: $\frac{\partial L_i}{\partial z_i} = \frac{\partial p_i}{\partial z_i} \frac{\partial L_i}{\partial p_i}$
    - $\frac{\partial L_i}{\partial p_i}$ 是一个列向量，其第 $j$ 个元素为 $-\frac{y_{ij}}{p_{ij}}$。
    - $\frac{\partial p_i}{\partial z_i}$ 是 $p_i$ 对 $z_i$ 的雅可比矩阵，这是一个 $D \times D$ 的矩阵。经过推导可以得到一个非常简洁的结果：
      $$
      \frac{\partial L_i}{\partial z_i} = p_i - y_i
      $$
    - 这个 `(预测概率 - 真实标签)` 的形式是 Softmax+交叉熵组合的一个非常优美的特性。

2.  **对 $W$ 求梯度**: 现在我们有了损失对线性得分的梯度，可以继续反向传播求对 $W$ 的梯度。
    - 考虑损失 $L_i$ 对 $W$ 的单个元素 $W_{jk}$ 的偏导数。
    - 使用链式法则: $\frac{\partial L_i}{\partial W_{jk}} = \frac{\partial L_i}{\partial z_{ij}} \frac{\partial z_{ij}}{\partial W_{jk}}$
    - 我们知道 $z_{ij} = \sum_k W_{jk} x_{ik}$。所以 $\frac{\partial z_{ij}}{\partial W_{jk}} = x_{ik}$。
    - $\frac{\partial L_i}{\partial W_{jk}} = (p_{ij} - y_{ij}) x_{ik}$

3.  **泛化为矩阵形式**:
    - 上式告诉我们，梯度矩阵 $\frac{\partial L_i}{\partial W}$ 的第 $(j,k)$ 个元素是 `误差项的第j个元素` 乘以 `输入的第k个元素`。
    - 这正好是一个**外积**的形式！
      $$
      \frac{\partial L_i}{\partial W} = (p_i - y_i) x_i^T
      $$
    - **维度检查**: $p_i - y_i$ 是 $C \times 1$，$x_i^T$ 是 $1 \times D$。外积结果是 $C \times D$，与 $W$ 的维度完全一致。

这个例子再次展示了链式法则和将元素级推导泛化为简洁矩阵形式的重要性。

---

## 9. 实用技巧与常见陷阱

1.  **维度检查是第一要务**: 在进行任何复杂的求导推导时，始终检查每一步操作后矩阵/向量的维度。维度不匹配是推导错误最常见的信号。链式法则 `(n x p) = (n x m) x (m x p)` 是你最好的朋友。

2.  **从元素开始，再泛化为矩阵**: 如果你不确定一个复杂的矩阵求导法则，可以先写出其任意一个元素 $(i,j)$ 的表达式，对该元素进行标量求导，然后观察结果的模式，最后将其泛化为完整的矩阵形式。

3.  **保持布局一致性**: 在一个推导过程中，绝对不要混合使用分子布局和分母布局。这会导致结果中出现莫名其妙的转置错误。

4.  **识别基本模式**: 许多复杂的表达式本质上是几个基本模式的组合，例如线性形式 ($a^Tx$) 和二次型 ($x^TAx$)。学会识别这些模式并直接套用公式可以大大加快推导速度。



